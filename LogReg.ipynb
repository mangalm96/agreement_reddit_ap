{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mangalm96/agreement_reddit_ap/blob/main/LogReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq0J5fpzaN5p"
      },
      "source": [
        "L2-regularized logistic regression for binary or multiclass classification; trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals and prints out the strongest coefficients for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "import math\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "scrolled": true,
        "outputId": "2518ab28-1b43-4175-f5e0-30ab134cb0db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have your folder of data on your Google drive account, you can connect that here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e9LBAOqSar4U",
        "outputId": "744430da-8939-4f77-da12-2693ed335335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to the directory with your data\n",
        "directory=\"/content/drive/MyDrive/Agreement_Reddit_AP/data/\""
      ],
      "metadata": {
        "id": "phhy5RpAasZY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-esP6NmXaN5u"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1].lstrip().rstrip()\n",
        "            text = cols[2]\n",
        "\n",
        "            X.append(text)\n",
        "            Y.append(label)\n",
        "\n",
        "    return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_reg = None\n",
        "\n",
        "        self.trainY=trainY\n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "        \n",
        "        self.trainX = self.process(trainX, training=True)\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "        \n",
        "        data = self.featurize(X_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def train(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        best_dev_accuracy=0\n",
        "        best_model=None\n",
        "        for C in [0.1, 1, 10, 100]:\n",
        "            self.log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "            self.log_reg.fit(self.trainX, self.trainY)\n",
        "            training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "            development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "            if development_accuracy > best_dev_accuracy:\n",
        "                best_dev_accuracy=development_accuracy\n",
        "                best_model=self.log_reg\n",
        "\n",
        "#             print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
        "\n",
        "        self.log_reg=best_model\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        return self.log_reg.score(self.testX, self.testY)\n",
        "        \n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Exd1Bwb2aN5w"
      },
      "outputs": [],
      "source": [
        "def binary_bow_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        feats[word]=1\n",
        "            \n",
        "    return feats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "43B4Pd1daN5w"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TkTQ7u0kaN5x"
      },
      "outputs": [],
      "source": [
        "def run(trainingFile, devFile, testFile):\n",
        "    trainX, trainY=load_data(trainingFile)\n",
        "    devX, devY=load_data(devFile)\n",
        "    testX, testY=load_data(testFile)\n",
        "    \n",
        "    simple_classifier = Classifier(binary_bow_featurize, trainX, trainY, devX, devY, testX, testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "    \n",
        "    lower, upper=confidence_intervals(accuracy, len(devY), .95)\n",
        "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
        "\n",
        "    simple_classifier.printWeights()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AU4ZHrkuaN5x",
        "outputId": "9ba4023b-ae36-42b4-98a0-831a319f7aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.755, 95% CIs: [0.695 0.815]\n",
            "\n",
            "Agreement\t0.218\tother\n",
            "Agreement\t0.217\tthat\n",
            "Agreement\t0.198\tif\n",
            "Agreement\t0.197\t&\n",
            "Agreement\t0.196\t%\n",
            "Agreement\t0.175\tleast\n",
            "Agreement\t0.164\tget\n",
            "Agreement\t0.164\tbefore\n",
            "Agreement\t0.159\t'd\n",
            "Agreement\t0.158\t3\n",
            "\n",
            "Disagreement\t0.321\tn't\n",
            "Disagreement\t0.288\ti\n",
            "Disagreement\t0.284\t?\n",
            "Disagreement\t0.266\tactually\n",
            "Disagreement\t0.233\tthis\n",
            "Disagreement\t0.217\tgraph\n",
            "Disagreement\t0.206\tsay\n",
            "Disagreement\t0.187\tsure\n",
            "Disagreement\t0.184\tsimply\n",
            "Disagreement\t0.182\tme\n",
            "\n",
            "Neutral\t0.389\tand\n",
            "Neutral\t0.293\tabstract\n",
            "Neutral\t0.252\twe\n",
            "Neutral\t0.221\tcovid-19\n",
            "Neutral\t0.216\tour\n",
            "Neutral\t0.200\tby\n",
            "Neutral\t0.178\t(\n",
            "Neutral\t0.175\t)\n",
            "Neutral\t0.166\t.\n",
            "Neutral\t0.162\thas\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainingFile = directory + \"train.txt\" \n",
        "devFile = directory + \"dev.txt\" \n",
        "testFile = directory + \"test.txt\"\n",
        "    \n",
        "run(trainingFile, devFile, testFile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### remove stop words"
      ],
      "metadata": {
        "id": "ZWqT4rfkgKax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "po7b9-FNgN7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_df(df):\n",
        "\n",
        "  # remove stopwords\n",
        "  filt_combined = []\n",
        "  for word in nltk.word_tokenize(df):\n",
        "\n",
        "    if word.lower() not in stopwords.words('english'):\n",
        "      filt_combined.append(word)\n",
        "\n",
        "  filtered_ip= \" \".join(filt_combined)\n",
        "\n",
        "  return filtered_ip"
      ],
      "metadata": {
        "id": "ZGbm1pGkgQo6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX, trainY=load_data(trainingFile)"
      ],
      "metadata": {
        "id": "dvIGBwIpgYWm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd = map(remove_stop_df, trainX)"
      ],
      "metadata": {
        "id": "hRvXi1vmhKOp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(dd)"
      ],
      "metadata": {
        "id": "xChXua4uhK3b",
        "outputId": "d593b72b-b9b9-4b76-f1e2-cde6778884a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-011f21e28500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-a53824a06ba9>\u001b[0m in \u001b[0;36mremove_stop_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mfilt_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "000yclZujPx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LogReg.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}